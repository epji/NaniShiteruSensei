import argparse
import pathlib
import requests


# =========================
# Ollama client
# =========================

def call_ollama(prompt: str,
                model: str = "llama3.1",
                host: str = "http://localhost:11434") -> str:
    """
    Call a local Ollama model with the given prompt and return the response text.
    Requires Ollama to be installed and running.
    """
    url = f"{host}/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
    }

    resp = requests.post(url, json=payload, timeout=600)
    resp.raise_for_status()
    data = resp.json()
    return data.get("response", "").strip()


# =========================
# Text utilities
# =========================

def load_text(path: pathlib.Path) -> str:
    return path.read_text(encoding="utf-8")


def chunk_text(text: str, max_chars: int = 3500) -> list[str]:
    """
    Roughly chunk text by character length so it fits in the model context.
    Tries to split on line boundaries.
    """
    text = text.replace("\r\n", "\n").replace("\r", "\n")

    chunks = []
    current = []
    current_len = 0

    for line in text.split("\n"):
        line = line.strip()
        if not line:
            line = ""

        if current_len + len(line) + 1 > max_chars and current:
            chunks.append("\n".join(current).strip())
            current = [line]
            current_len = len(line) + 1
        else:
            current.append(line)
            current_len += len(line) + 1

    if current:
        chunks.append("\n".join(current).strip())

    return [c for c in chunks if c]


# =========================
# Summarization pipeline
# =========================

def summarize_chunk(chunk_text: str,
                    part_idx: int,
                    total_parts: int,
                    model: str,
                    host: str) -> str:
    """
    Ask the model for a short bullet summary of one chunk.
    """
    prompt = f"""ã‚ãªãŸã¯æ—¥æœ¬ã®å¤§å­¦è¬›ç¾©ãƒãƒ¼ãƒˆã‚’ä½œã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã¯è¬›ç¾©æ–‡å­—èµ·ã“ã—ã®ä¸€éƒ¨ã§ã™ã€‚ï¼ˆPart {part_idx}/{total_parts}ï¼‰

ã“ã®éƒ¨åˆ†ã ã‘ã«ã¤ã„ã¦ã€å¾Œã§å…¨ä½“ã‚’ã¾ã¨ã‚ã‚‹ãŸã‚ã®ã€Œãƒ¡ãƒ¢ç”¨ã®ç®‡æ¡æ›¸ãã€ã‚’ä½œã£ã¦ãã ã•ã„ã€‚

æ¡ä»¶:
- æ—¥æœ¬èªã§æ›¸ã
- ç®‡æ¡æ›¸ã 3ã€œ6 å€‹
- é‡è¦ãªç”¨èªãƒ»äººåãƒ»åœ°åãƒ»æ™‚ä»£åã¯ã§ãã‚‹ã ã‘ãã®ã¾ã¾æ®‹ã™
- ç°¡æ½”ã«è¦ç‚¹ã ã‘ã¾ã¨ã‚ã‚‹
- æ–‡ç« ã®é †ç•ªã¯ã€æµã‚ŒãŒåˆ†ã‹ã‚Šã‚„ã™ã„ã‚ˆã†ã«ä¸¦ã¹ã‚‹

=== Transcript Part {part_idx}/{total_parts} ===
{chunk_text}
==========================================

å‡ºåŠ›ã¯ã€æ—¥æœ¬èªã®ç®‡æ¡æ›¸ãã ã‘ã«ã—ã¦ãã ã•ã„ã€‚
ã€ŒPart â—‹â—‹ã€ãªã©ã®ä½™è¨ˆãªãƒ˜ãƒƒãƒ€ãƒ¼ã‚„èª¬æ˜æ–‡ã¯æ›¸ã‹ãªã„ã§ãã ã•ã„ã€‚
"""

    return call_ollama(prompt, model=model, host=host)


def build_final_notes_from_summaries(summaries: list[str],
                                     model: str,
                                     host: str) -> str:
    """
    Given a list of chunk-level summaries, ask the model to create
    the full structured lecture notes (JP only).
    """
    merged_summaries = "\n\n".join(
        f"[Part {i+1}]\n{summaries[i]}"
        for i in range(len(summaries))
    )

    prompt = f"""ã‚ãªãŸã¯ã€æ—¥æœ¬ã®å¤§å­¦ã§å‹‰å¼·ã—ã¦ã„ã‚‹ç•™å­¦ç”Ÿã®ãŸã‚ã®
ã€Œåˆ†ã‹ã‚Šã‚„ã™ã„è¬›ç¾©ãƒãƒ¼ãƒˆä½œæˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€ã§ã™ã€‚

ä»¥ä¸‹ã«ã€è¬›ç¾©å…¨ä½“ã®è¦ç´„ãƒ¡ãƒ¢ï¼ˆå„ãƒ‘ãƒ¼ãƒˆã”ã¨ã®ç®‡æ¡æ›¸ãï¼‰ãŒä¸ãˆã‚‰ã‚Œã¾ã™ã€‚
ã“ã‚Œã‚‰ã‚’ã‚‚ã¨ã«ã€ï¼‘ã¤ã®è¬›ç¾©ãƒãƒ¼ãƒˆã‚’ä½œã£ã¦ãã ã•ã„ã€‚

å­¦ç”Ÿã¯:
- æ—¥æœ¬èªãŒç¬¬ï¼“è¨€èª
- ãƒ†ã‚¹ãƒˆå¯¾ç­–ã¨ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã«ä½¿ã„ãŸã„
- æ­´å²ã‚„æ¦‚å¿µã®å› æœé–¢ä¿‚ã‚’ç†è§£ã—ãŸã„
- é‡è¦ãªç”¨èªã¯æ—¥æœ¬èªã§ã—ã£ã‹ã‚Šè¦šãˆãŸã„

å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯**å¿…ãš**æ¬¡ã®è¦‹å‡ºã—ãƒ»é †ç•ªã§ä½œã£ã¦ãã ã•ã„ã€‚
è¦‹å‡ºã—ã¯ã€Œ### è¦‹å‡ºã—åã€ã®å½¢ã§æ›¸ã„ã¦ãã ã•ã„ã€‚

1. ä»Šæ—¥ã®ãƒ†ãƒ¼ãƒ
2. é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
3. ç†è§£ãƒã‚¤ãƒ³ãƒˆ
4. è©¦é¨“ã«å‡ºãã†ãªã¨ã“ã‚
5. Definitions Cheat Sheet
6. è¦ç‚¹5è¡Œã¾ã¨ã‚
7. Q&A Flashcards
8. ãƒ¡ãƒ¢ï¼šå…ˆç”Ÿã®å¼·èª¿ãƒã‚¤ãƒ³ãƒˆ

ãã‚Œãã‚Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æŒ‡ç¤º:

### ä»Šæ—¥ã®ãƒ†ãƒ¼ãƒ
- è¬›ç¾©å…¨ä½“ãŒä½•ã«ã¤ã„ã¦ã ã£ãŸã‹ã‚’ã€æ—¥æœ¬èªã§2ã€œ4æ–‡ã§ã¾ã¨ã‚ã‚‹ã€‚

### é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
- ç®‡æ¡æ›¸ãã€‚
- å½¢å¼ã®ä¾‹:
  ãƒ»ç”¨èª: æ—¥æœ¬èªã§1ã€œ2è¡Œã®èª¬æ˜
- äººåãƒ»åœ°åãƒ»æ™‚ä»£åãƒ»æ¦‚å¿µã‚’ä¸­å¿ƒã«ã€‚

### ç†è§£ãƒã‚¤ãƒ³ãƒˆ
- æ—¥æœ¬èªã§ã€å› æœé–¢ä¿‚ã‚„æµã‚ŒãŒåˆ†ã‹ã‚‹ã‚ˆã†ã«èª¬æ˜ã€‚
- ç®‡æ¡æ›¸ãã§ã‚‚çŸ­ã„æ®µè½ã§ã‚‚OKã€‚
- ã€Œãªãœã€œãŒèµ·ããŸã‹ã€ã€Œã©ã®å‹¢åŠ›ãŒã©ã†å‹•ã„ãŸã‹ã€ã€Œçµæœã©ã†ãªã£ãŸã‹ã€ã‚’æ„è­˜ã™ã‚‹ã€‚

### è©¦é¨“ã«å‡ºãã†ãªã¨ã“ã‚
- è¨˜è¿°å¼ã‚„å°è«–æ–‡ã§å‡ºãã†ãªè¨­å•ã‚’æ—¥æœ¬èªã§3ã€œ6å€‹ä½œã‚‹ã€‚
- ã€Œã€œã«ã¤ã„ã¦èª¬æ˜ã›ã‚ˆã€ã€Œã€œã®ç‰¹å¾´ã‚’è¿°ã¹ã‚ˆã€ãªã©ã®å½¢ã€‚

### Definitions Cheat Sheet
- ç”¨èªè¾æ›¸ï¼ˆè¶…çŸ­ãï¼‰ã€‚
- å½¢å¼:
  ç”¨èª: ä¸€è¨€ã§æ„å‘³ï¼ˆæ—¥æœ¬èªï¼‰
- 5ã€œ10èªç¨‹åº¦ã§OKã€‚

### è¦ç‚¹5è¡Œã¾ã¨ã‚
- æ—¥æœ¬èªã§5ã¤ã®æ–‡ã«ã¾ã¨ã‚ã‚‹ã€‚
- â‘ ã€œâ‘¤ã®ç•ªå·ä»˜ãã§ã€‚

### Q&A Flashcards
- æš—è¨˜ç”¨ã®è³ªå•ã¨ç­”ãˆã‚’3ã€œ8ã‚»ãƒƒãƒˆä½œã‚‹ã€‚
- å½¢å¼:
  Q: ï¼ˆæ—¥æœ¬èªã®è³ªå•ï¼‰
  A: ï¼ˆæ—¥æœ¬èªã®ç­”ãˆï¼‰

### ãƒ¡ãƒ¢ï¼šå…ˆç”Ÿã®å¼·èª¿ãƒã‚¤ãƒ³ãƒˆ
- è¬›ç¾©ã®ä¸­ã§å¼·èª¿ã•ã‚Œã¦ã„ãã†ãªãƒ†ãƒ¼ãƒã‚„ã€ä½•åº¦ã‚‚å‡ºã¦ããŸè€ƒãˆæ–¹ã‚’æ—¥æœ¬èªã§ç®‡æ¡æ›¸ãã€‚

--------------------------------
ã€è¬›ç¾©ãƒ¡ãƒ¢ï¼ˆãƒ‘ãƒ¼ãƒˆã”ã¨ï¼‰ã€‘
{merged_summaries}
--------------------------------

ãã‚Œã§ã¯ã€æŒ‡ç¤ºã—ãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¾“ã£ã¦è¬›ç¾©ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
"""

    return call_ollama(prompt, model=model, host=host)


# =========================
# Main CLI
# =========================

def main():
    parser = argparse.ArgumentParser(
        description="Generate structured JP-only lecture notes from a Japanese transcript using a local Ollama model."
    )
    parser.add_argument(
        "transcript_file",
        type=str,
        help="Path to the transcript .txt file (e.g., Week 7_transcript_ja.txt)",
    )
    parser.add_argument(
        "--model",
        type=str,
        default="llama3.1",
        help="Ollama model name (default: llama3.1)",
    )
    parser.add_argument(
        "--host",
        type=str,
        default="http://localhost:11434",
        help="Ollama host URL (default: http://localhost:11434)",
    )
    parser.add_argument(
        "--chunk-chars",
        type=int,
        default=3500,
        help="Approximate max characters per chunk before summarizing (default: 3500)",
    )

    args = parser.parse_args()

    transcript_path = pathlib.Path(args.transcript_file)
    if not transcript_path.exists():
        print(f"File not found: {transcript_path}")
        return

    print(f"ğŸ“– Loading transcript: {transcript_path.name}")
    text = load_text(transcript_path)

    print("ğŸ”ª Splitting transcript into chunks...")
    chunks = chunk_text(text, max_chars=args.chunk_chars)
    print(f"  â†’ {len(chunks)} chunk(s)")

    # 1) Summarize each chunk
    summaries = []
    for i, chunk in enumerate(chunks, start=1):
        print(f"ğŸ“ Summarizing chunk {i}/{len(chunks)} ...")
        summary = summarize_chunk(
            chunk,
            part_idx=i,
            total_parts=len(chunks),
            model=args.model,
            host=args.host,
        )
        summaries.append(summary)

    # 2) Build final notes
    print("ğŸ“š Building final structured notes from chunk summaries...")
    final_notes = build_final_notes_from_summaries(
        summaries,
        model=args.model,
        host=args.host,
    )

    # 3) Save output
    out_path = transcript_path.with_name(
        transcript_path.stem + "_NOTES_ollama.txt"
    )
    out_path.write_text(final_notes, encoding="utf-8")

    print("âœ… Done!")
    print(f"Notes saved to: {out_path}")


if __name__ == "__main__":
    main()
